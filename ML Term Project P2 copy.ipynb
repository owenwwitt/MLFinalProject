{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ML_utils\n",
    "import torch as nn\n",
    "\n",
    "path2data = \"./data\"\n",
    "sub_folder_jpg = \"hmdb51_jpg\"\n",
    "jpg_path = os.path.join(path2data, sub_folder_jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6766, 6766, 51)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calls get_vids to get a list of all the videos, labels, and categories in the dataset.\n",
    "all_vids, all_labels, catgs = ML_utils.get_vids(jpg_path) \n",
    "len(all_vids), len(all_labels), len(catgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Now we have our data in jpg form, but this still isn't good enough to feed into a ML model. Here, I call get_vids to go into all the subdirectories of jpg_path(our video folder) and makes the full path between categories and jpg_path. If the item being looked at is in fact a directory and doesn't start with '.' (I had some problems with the mac OS and a file called .DS Store), we also create full paths between frames and jpg_path. Lastly, we output a list of all paths to frames and their corresponding labels (subdirectory name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dictionary of the categories and their corresponding labels.\n",
    "labels_dict = {}\n",
    "ind = 0\n",
    "for cat in catgs:\n",
    "    if cat.startswith('.'):\n",
    "        continue\n",
    "    labels_dict[cat] = ind\n",
    "    ind+=1\n",
    "labels_dict\n",
    "\n",
    "#Creates a reverse dictionary\n",
    "index_to_label = {v: k for k, v in labels_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 233)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 2       #Testing that methods work for the first 5 classes (may switch back if I buy GPU).\n",
    "#Adds all ids and labels for the first *num_classes* classes to a list.\n",
    "unique_ids = [id_ for id_, label in zip(all_vids,all_labels) if labels_dict[label]<num_classes]\n",
    "unique_labels = [label for id_, label in zip(all_vids,all_labels) if labels_dict[label]<num_classes]\n",
    "len(unique_ids),len(unique_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Here, we take our categories and turn them into a dictionary where the keys are the label names and the values are a corresponding number 0-51. I also create a reverse dictionary to help make sure that the datasets I create below work as intended.\n",
    "\n",
    "We then set up a small block of code that lets us control the number of classes that we will use for our model later. I am not able to run this whole dataset on my computer so I tried downsizing to a smaller number of classes to see if that would help. I may end up renting a GPU for this though, tbd. I check to make sure the number of samples is still the same, then divide the data into 3 folds of training and testing data via sklearn's StratifiedShuffleSplit. I set the number of splits to be 3, 10% of data to be test data, and random_state to be 0 in order for my results to be reproducible. StratifiedShuffleSplit separates the data into different splits while maintaining prior probabilities. It also shuffles the data so the model doesn't rely on any kind of information between samples. Test sets will not necessarily be completely mutually exlusive, but they will be close(sklearn).\n",
    "\n",
    "After this is done, we iterate over our splits and create lists of training/testing videos and labels with shape [#splits, #samples in category of split]. I then check that the first 5 elements of the first trainign split to make sure it still is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "#Random seed for reproducibility\n",
    "np.random.seed(200)\n",
    "random.seed(200)\n",
    "torch.manual_seed(200)\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    #Constructor\n",
    "    def __init__(self, ids, labels, transform):      \n",
    "        self.transform = transform\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "    #Length\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    #Getter\n",
    "    def __getitem__(self, idx):\n",
    "        #Get all the frames in the video and corresponding label\n",
    "        path2imgs=glob.glob(self.ids[idx]+\"/*.jpg\")\n",
    "        path2imgs = path2imgs[:16]\n",
    "        label = labels_dict[self.labels[idx]]\n",
    "        frames = []\n",
    "        #For each frame, open the image and apply the transform\n",
    "        for path2img in path2imgs:\n",
    "            frame = Image.open(path2img)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        seed = np.random.randint(1e9)        \n",
    "        frame_out = []\n",
    "        #Apply the same seed to all frames in the video\n",
    "        for frame in frames:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            frame = self.transform(frame)\n",
    "            frame_out.append(frame)\n",
    "        if len(frame_out)>0:\n",
    "            frame_out = torch.stack(frame_out) #Turns frames into tensor [num_frames, channels, height, width]\n",
    "        return frame_out, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Here, I am creating a dataset that I can use to put in my dataloader later. I set seeds to make my outputs reproducible here as well. The dataset is defined by the ids and labels I created earlier, and a set of transforms I define below. \n",
    "\n",
    "__len__ returns the length of the dataset. \n",
    "\n",
    "__getitem__ uses glob to get a list of all the jpg files in the directory (in case any aren't jpg files), and takes the first 16 frames (again all videos should be 16 frames already but just in case). We get the label for the image using the label dictionary created earlier. We then open all frames in the video using PIL and add them to a frames list. We randomly transform each frame, and add it to our output list. Once this is done, we turn the output list into a tensor of shape [num_frames, channels, height, width] and output it with the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two models:\n",
    "model_type = \"3dcnn\"\n",
    "#model_type = \"rnn\"    \n",
    "\n",
    "if model_type == \"rnn\":\n",
    "    h, w =224, 224\n",
    "    mean = [0.485, 0.456, 0.406] #Weights for RNN pretrained on ImageNet \n",
    "    std = [0.229, 0.224, 0.225] #Weights for RNN pretrained on ImageNet\n",
    "if model_type == \"3dcnn\":\n",
    "    h, w = 112, 112\n",
    "    mean = [0.43216, 0.394666, 0.37645] #Weights for 3D-CNN pretrained on Kinetics\n",
    "    std = [0.22803, 0.22145, 0.216989]  #Weights for 3D-CNN pretrained on Kinetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Here, I give two options for model_type, 3D-CNN, or RNN. To use either one, you can # out / leave in the model_type = \"rnn\" line. I chose to pretrain my models in order to take advantage of transfer learning and have them converge faster as I don't have unlimited GPU power. I learned about this at my internship over the summer and it is perfect here as this is very similar to an object classification task. I found a pytorch page that recommended the 'else' weights which were found using the Kinetics dataset. The RNN needed be downsized to 224x224 because this is the default size needed for the base model I used(ResNet18). I downsized the 3DCNN more than the RNN because its more robust in order to have comparable training times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_transformer = transforms.Compose([\n",
    "            transforms.Resize((h,w)),   #Resize to desired size\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  #Flip image half the time, makes model more robust\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),   #Translate 10% in any direction\n",
    "            transforms.ToTensor(),  #Turns to tensor\n",
    "            transforms.Normalize(mean, std), #Normalize\n",
    "            ])     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "These are the transformations I mentioned earlier. These transformations were recommended by KeyLabs.ai. I didn't want to get too crazy so I stuck with geometric transformations and didn't mess with rotation. Overall, this downsizes our image depending on the method chosen, has a 50% chance to flip the image, translates it up to 10% in any direction, converts the image from the PIL image it was opened as to a tensor, then normalizes the color channels by their corresponding mean and std."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Creates training dataset using splits and transforms from above. Flattens training data beforehand so Dataset can work as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simpler version of training, don't need to worry about training \n",
    "test_transformer = transforms.Compose([\n",
    "            transforms.Resize((h,w)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Above, I spend some time making sure that our training images and labels still match. I also do a similar transformation with the test frames except I just downsize them, turn them into tensors, and normalize them (no extra transformations necessary as no learning). I then flatten the test data and put it in its own test dataset. I then make sure our testing images and labels still match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define collate functions for the Dataloaders\n",
    "def collate_fn_3DCNN(batch):\n",
    "    imgs_batch, label_batch = list(zip(*batch))\n",
    "    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]\n",
    "    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]\n",
    "    imgs_tensor = torch.stack(imgs_batch)\n",
    "    imgs_tensor = torch.transpose(imgs_tensor, 2, 1) #Turns into tensor of size (batch_size, height, channels, width) for CNN\n",
    "    labels_tensor = torch.stack(label_batch)\n",
    "    return imgs_tensor,labels_tensor\n",
    "\n",
    "def collate_fn_rnn(batch):\n",
    "    imgs_batch, label_batch = list(zip(*batch))\n",
    "    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]\n",
    "    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]\n",
    "    imgs_tensor = torch.stack(imgs_batch)\n",
    "    labels_tensor = torch.stack(label_batch)\n",
    "    return imgs_tensor,labels_tensor\n",
    "    \n",
    "\n",
    "batch_size = 1\n",
    "#Load data into Dataloader\n",
    "\n",
    "def load_data(train_ds, test_ds, model_type):\n",
    "    if model_type == \"rnn\":\n",
    "        train_dl = DataLoader(train_ds, batch_size= batch_size,\n",
    "                            shuffle=True, collate_fn= collate_fn_rnn)\n",
    "        test_dl = DataLoader(test_ds, batch_size= 2*batch_size,\n",
    "                            shuffle=False, collate_fn= collate_fn_rnn)  \n",
    "    if model_type == \"3dcnn\":\n",
    "        train_dl = DataLoader(train_ds, batch_size= batch_size, \n",
    "                            shuffle=True, collate_fn= collate_fn_3DCNN)\n",
    "        test_dl = DataLoader(test_ds, batch_size= 2*batch_size, \n",
    "                            shuffle=False, collate_fn= collate_fn_3DCNN)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Here, I make different collate functions for the RNN and CNN. I make these to make sure our batches are going to be handled properly in our dataloader. Most of the steps are the same for both. When given a batch, which I've set to 1, we unzip the batch into images and labels. We then take all the images into a list and convert the labels into a tensor. We stack the images into a tensor of size (batch_size,channels, height, width) and the labels into a tensor of size (batch_size, 1). CNNs need a shape of (batch_size, height, channels, width) though, so we transpose the 1st and 2nd dimensions(considering batch_size to be dimension 0). We then return these tensors.\n",
    "\n",
    "After this, we load our data into their corresponding DataLoaders, differentiated by their collate functions. We set shuffle to be true for training and false for testing. Although this isn't necessarily needed as we shuffled earlier, it doesn't hurt. \n",
    "\n",
    "Below, we check the dimensions of the active model's dataloader and make sure its the right size. Notice we doubled the batch size of the testing loader since its less computationally complex so the first dimension is 2 compared to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Checks that dataloader is of correct size. It is so we're good to keep going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlines Resnet18 model with LSTM\n",
    "import torch.nn as nn\n",
    "class Resnet18Rnn(nn.Module):\n",
    "    #Constructor\n",
    "    def __init__(self, params_model):\n",
    "        #Parameters\n",
    "        super(Resnet18Rnn, self).__init__()\n",
    "        num_classes = params_model[\"num_classes\"]\n",
    "        dr_rate= params_model[\"dr_rate\"]\n",
    "        pretrained = params_model[\"pretrained\"]\n",
    "        rnn_hidden_size = params_model[\"rnn_hidden_size\"]\n",
    "        rnn_num_layers = params_model[\"rnn_num_layers\"]\n",
    "        #Layer definitions\n",
    "        baseModel = models.resnet18(pretrained=pretrained) #Gets pretrained resnet18 model\n",
    "        num_features = baseModel.fc.in_features\n",
    "        baseModel.fc = PassThrough()\n",
    "        self.baseModel = baseModel\n",
    "        self.dropout= nn.Dropout(dr_rate)\n",
    "        self.rnn = nn.LSTM(num_features, rnn_hidden_size, rnn_num_layers)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, num_classes)\n",
    "    #Forward pass\n",
    "    def forward(self, x):\n",
    "        b_z, ts, c, h, w = x.shape\n",
    "        i = 0\n",
    "        y = self.baseModel((x[:,i]))  \n",
    "        output, (hn, cn) = self.rnn(y.unsqueeze(1)) #only care about final output\n",
    "        #Learns probabilities through iterating through frames in a video\n",
    "        for i in range(1, ts):\n",
    "            y = self.baseModel((x[:,i]))\n",
    "            out, (hn, cn) = self.rnn(y.unsqueeze(1), (hn, cn))\n",
    "        out = self.dropout(out[:,-1])\n",
    "        out = self.fc1(out) \n",
    "        return out \n",
    "    \n",
    "class PassThrough(nn.Module): #Remove final layer of resnet18 model so we can input into LSTM instead. \n",
    "    def __init__(self):\n",
    "        super(PassThrough, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: RNN\n",
    "\n",
    "There's already been some talk about the different preprocessing steps I have to do depending on the model, but here is the bulk of my RNN code. Here, I initialize my model with a number of parameters such as #classes, dropout rate, whether my model is pretrained, the hidden size of the model, and #parameters. I also initialize the base model of ResNet which mostly consists of a combination of convolution layers, pooling layers, and residual blocks. I can go into more detail on how this works during Q&A if desired. We take the number of features from this base model(512) as well. \n",
    "\n",
    "ResNet is good at handling 2D data, but we need to add another model in order for it to handle temporal data. To do this, we remove the final fully connected layer of the model through the use of the PassThrough class and instead feed directly to an LSTM (after a dropout layer). This LSTM handles temporal data by keeping a hidden state that holds information about its past states. The RNN features from earlier are inputted here, and I chose to use only 1 layer with a hidden size of 128 to keep the model relatively lightweight while still hopefully being able to get temporal data. An article I read from medium recommend a size of 128 for image classification, so that's what I used. Finally, I have a linear layer to give class probabilities.\n",
    "\n",
    "forward: Iterates first frame through ResNet model and LSTM, outputting the predicted class, the hidden state of the LSTM, and the cell state of the LSTM. We then iterate through the rest of the frames until we reach the end. We get a final output from the LSTM which we then do dropout on for regularization. Finally, we run the output through a linear layer to get the class probabilities which we return. We have to unsqueeze the output y because we need the input to be of the form (batch_size, seq_len, input_size). Since we have are only iterating over one from at a time, we set seq_len to be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: 3D-CNN\n",
    "\n",
    "The second model I chose to use was a pretrained 3D-CNN. I pulled this model from the pytorch models directory. This model works very similarly to ResNet as it also uses mostly convolutional and pooling layers with RELU, dropout, etc. The biggest difference is that while ResNet works in two dimensions, this model works in 3D. In that sense, instead of going frame by frame to get an output, it takes the entire video as an input where the third dimension is time. Similar to before, we set the last layer of this model to be a linear layer that connects the in_features (values right before the final layer) to the a number of nodes equal to the number of class(51 in our case). In this way, our output will be an array of the probabilities of each class. As mentioned before, if you'd like me to go into more detail during Q/A I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenwitt/Desktop/Guided Research/Guided Research Code/First Neural Network/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/owenwitt/Desktop/Guided Research/Guided Research Code/First Neural Network/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "#Defines paramters based on model type\n",
    "if model_type == \"rnn\":\n",
    "    params_model={\n",
    "        \"num_classes\": num_classes,\n",
    "        \"dr_rate\": 0.1,\n",
    "        \"pretrained\" : True,\n",
    "        \"rnn_num_layers\": 1,\n",
    "        \"rnn_hidden_size\": 128,}\n",
    "    model = Resnet18Rnn(params_model)        \n",
    "if model_type == \"3dcnn\":\n",
    "    model = models.video.r3d_18(pretrained=True, progress=False)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing\n",
    "Denotes the model and its corrresponding parameters depending on the model_type variable mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Checking\n",
    "\n",
    "Checks that output is of the correct size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #Good practice\n",
    "model = model.to(device)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2weights = \"./models/weights\"\n",
    "torch.save(model.state_dict(), path2weights + \"_\" + model_type + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Weights\n",
    "\n",
    "Saves starting weights of model. Will be overwritten below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenwitt/Desktop/Guided Research/Guided Research Code/First Neural Network/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "#Defines loss function, optimizer, and learning rate scheduler\n",
    "loss_func = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "opt = optim.Adam(model.parameters(), lr=3e-5)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min',factor=0.5, patience=5,verbose=1)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 155, Test set size: 78\n",
      "Epoch 0/19, current lr=3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [02:26<00:00,  1.06it/s]\n",
      "100%|██████████| 39/39 [00:21<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied best model weights!\n",
      "train loss: 0.708258, dev loss: 0.883289, accuracy: 47.44\n",
      "----------\n",
      "Epoch 1/19, current lr=3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/155 [00:11<02:28,  1.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fold_results\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#Run k-fold cross validation\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mk_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(model, k)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m#Defines parameters for training\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     params_train\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: opt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath2weights\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/weights_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_type\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m }\n\u001b[0;32m---> 39\u001b[0m     model_copy,loss_hist,metric_hist \u001b[38;5;241m=\u001b[39m \u001b[43mML_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_hist, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m: metric_hist})\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fold_results\n",
      "File \u001b[0;32m~/Desktop/Guided Research/Guided Research Code/First Neural Network/ML_utils.py:62\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, current lr=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, current_lr))\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 62\u001b[0m train_loss, train_metric\u001b[38;5;241m=\u001b[39m\u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43msanity_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     64\u001b[0m metric_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metric)\n",
      "File \u001b[0;32m~/Desktop/Guided Research/Guided Research Code/First Neural Network/ML_utils.py:118\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[1;32m    116\u001b[0m yb\u001b[38;5;241m=\u001b[39myb\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m#Good practice\u001b[39;00m\n\u001b[1;32m    117\u001b[0m output\u001b[38;5;241m=\u001b[39mmodel(xb)\n\u001b[0;32m--> 118\u001b[0m loss_b,metric_b\u001b[38;5;241m=\u001b[39m\u001b[43mloss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#loss_b in the overall loss for the batch, metric_b is the number of correct predictions\u001b[39;00m\n\u001b[1;32m    119\u001b[0m running_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss_b\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Guided Research/Guided Research Code/First Neural Network/ML_utils.py:105\u001b[0m, in \u001b[0;36mloss_batch\u001b[0;34m(loss_func, output, target, opt)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem(), metric_b\n",
      "File \u001b[0;32m~/Desktop/Guided Research/Guided Research Code/First Neural Network/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Guided Research/Guided Research Code/First Neural Network/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Guided Research/Guided Research Code/First Neural Network/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import copy\n",
    "\n",
    "def k_fold_cross_validation(model, k=3):\n",
    "\n",
    "    #Splits data in a stratified & shuffled way\n",
    "    sss = StratifiedShuffleSplit(n_splits=k, test_size=1/k, random_state=0) #Multiple splits\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    for train_indx, val_indx in sss.split(unique_ids, unique_labels):\n",
    "        train_ids = [unique_ids[ind] for ind in train_indx]\n",
    "        train_labels = [unique_labels[ind] for ind in train_indx]\n",
    "        val_ids = [unique_ids[ind] for ind in val_indx]\n",
    "        val_labels = [unique_labels[ind] for ind in val_indx]\n",
    "        \n",
    "        # Optionally print information about the split\n",
    "        print(f\"Train set size: {len(train_ids)}, Test set size: {len(val_ids)}\")\n",
    "\n",
    "        train_ds = VideoDataset(ids= train_ids, labels= train_labels, transform= train_transformer) #Create dataset\n",
    "        val_ds = VideoDataset(ids= val_ids, labels= val_labels, transform= test_transformer) #Create dataset\n",
    "\n",
    "        train_dl, val_dl = load_data(train_ds, val_ds, model_type)\n",
    "        \n",
    "        model_copy = copy.deepcopy(model)\n",
    "\n",
    "        #Defines parameters for training\n",
    "        params_train={\n",
    "            \"num_epochs\": 20,\n",
    "            \"optimizer\": opt,\n",
    "            \"loss_func\": loss_func,\n",
    "            \"train_dl\": train_dl,\n",
    "            \"val_dl\": val_dl,\n",
    "            \"sanity_check\": False,  #Set to True to check the forward pass and the loss\n",
    "            \"lr_scheduler\": lr_scheduler,\n",
    "            \"path2weights\": \"./models/weights_\"+model_type+\".pt\",\n",
    "    }\n",
    "\n",
    "        model_copy,loss_hist,metric_hist = ML_utils.train_val(model_copy,params_train)\n",
    "\n",
    "        fold_results.append({\"loss_hist\": loss_hist, \"metric_hist\": metric_hist})\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "#Run k-fold cross validation\n",
    "k_fold_cross_validation(model, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Models\n",
    "\n",
    "Start of by setting loss function to be Cross Entropy Loss, our optimizer to be the Adam optimizer, and our learning rate scheduler to be ReduceLROnPlateau. I chose cross entropy loss as it grades harder based on the 'wrongness' of the model, and is a staple of deep learning models and image/video classification. I chose the Adam Optimizer because of its commonplace in these tasks as well and is generally good at a lot of tasks. I am using the ReduceLROnPlateau learning rate scheduler becasue I'm fine tuning pre-trained models. It's common for the validation to plateau when fine tuning, so I've added ReduceLROnPlateau to reduce the learning rate when that occurs. This will allow our models to converge to the best possible answer as they won't endlessly oscillate due to overshooting from a too high learning rate. Also lets me be more lenient in what I choose as my learning rate.\n",
    "\n",
    "We then feed the parameters, our training and testing datasets, and the number of epochs into our model (among other things). I chose the nuber of epochs to be 50 for both models as according to various sources I would need anywhere from a few dozen to a few hundred epochs. As I don't have the time to do a few hundred, I chose the lesser end. \n",
    "\n",
    "Our model is ran through the train_val function. This is a relatively straightforward learning/validation cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ML_utils\u001b[38;5;241m.\u001b[39mplot_loss(\u001b[43mloss_hist\u001b[49m, metric_hist)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_hist' is not defined"
     ]
    }
   ],
   "source": [
    "ML_utils.plot_loss(loss_hist, metric_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Models\n",
    "Tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Discussion\n",
    "\n",
    "One ethical implication of using the HMDB51 database is privacy concerns. A large majority of the videos in this database were gathered from Youtube, and the people taking part did not consent to be used in the database. The dataset also could contain biases relating to race, gender, or socioeconomics. If certain groups are over/underrepresented, the model could perform better on some groups than others, leading to bias. \n",
    "\n",
    "One particularly harmful example could be of the use of video classification in police drones. A drone could classify someone as holding a gun or mugging someone when this isn't the case at all. The drone may decide to take action and could potentially harm/profile someone who is innocent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
